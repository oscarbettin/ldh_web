#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Cliente para integraci√≥n con Google Gemini API
Especializado en an√°lisis de im√°genes m√©dicas
"""

import os
import json
import requests
import base64
from typing import Dict, List, Optional, Any
import logging

logger = logging.getLogger(__name__)

class GeminiClient:
    """Cliente para interactuar con Google Gemini API"""
    
    def __init__(self, modelo=None):
        # Intentar obtener API key desde variable de entorno primero
        self.api_key = os.getenv('GEMINI_API_KEY')
        
        # Si no est√° en variable de entorno, intentar desde Flask config (para desarrollo)
        if not self.api_key:
            try:
                from flask import current_app
                self.api_key = current_app.config.get('GEMINI_API_KEY', '')
            except:
                pass
        
        # Si a√∫n no hay API key, intentar leerla desde config.py directamente (fallback)
        if not self.api_key:
            try:
                from config import Config
                config = Config()
                self.api_key = getattr(config, 'GEMINI_API_KEY', '')
            except:
                pass
        
        # Modelos disponibles:
        # - gemini-2.5-flash (estable, multimodal, r√°pido) - RECOMENDADO
        # - gemini-2.0-flash (estable, multimodal, r√°pido)
        # - gemini-2.5-pro (estable, multimodal, m√°s potente)
        # - gemini-pro-vision (legacy, siempre disponible)
        
        # Intentar obtener modelo desde Flask config
        try:
            from flask import current_app
            modelo_config = current_app.config.get('GEMINI_MODEL')
        except:
            modelo_config = None
        
        if modelo:
            self.model = modelo
        elif modelo_config:
            self.model = modelo_config
        else:
            self.model = os.getenv('GEMINI_MODEL', "gemini-2.5-flash")  # Por defecto usar gemini-2.5-flash (multimodal)
        
        # Base URL para Gemini API
        # Intentar primero con v1beta (m√°s reciente), si falla usar v1
        # Formato: https://generativelanguage.googleapis.com/v1beta/models/{modelo}:generateContent
        self.base_url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model}:generateContent"
        self.base_url_v1 = f"https://generativelanguage.googleapis.com/v1/models/{self.model}:generateContent"
        
        # Log del modelo seleccionado
        modelo_origen = "par√°metro" if modelo else ("config.py" if modelo_config else "variable de entorno o default")
        logger.info(f"ü§ñ GeminiClient inicializado - Modelo: {self.model} (origen: {modelo_origen})")
    
    def is_configured(self) -> bool:
        """Verificar si la API key est√° configurada"""
        return bool(self.api_key)
    
    def listar_modelos_disponibles(self) -> List[Dict]:
        """
        Listar modelos disponibles en Gemini API
        """
        if not self.api_key:
            return []
        
        modelos_disponibles = []
        
        # Intentar con v1beta primero
        for version in ['v1beta', 'v1']:
            try:
                url = f"https://generativelanguage.googleapis.com/{version}/models"
                params = {"key": self.api_key}
                
                response = requests.get(url, params=params, timeout=10)
                response.raise_for_status()
                data = response.json()
                
                if 'models' in data:
                    for modelo in data['models']:
                        nombre = modelo.get('name', '')
                        # Extraer solo el nombre del modelo (sin el prefijo "models/")
                        if 'models/' in nombre:
                            nombre = nombre.split('models/')[1]
                        
                        metodos = modelo.get('supportedGenerationMethods', [])
                        
                        # Los modelos Gemini 2.0+ son multimodales por defecto (soportan visi√≥n)
                        # Tambi√©n modelos con "vision" en el nombre
                        es_multimodal = (
                            'vision' in nombre.lower() or 
                            'pro-vision' in nombre.lower() or
                            nombre.startswith('gemini-2.') or  # Gemini 2.0+ son multimodales
                            nombre.startswith('gemini-3.') or  # Gemini 3.0+ son multimodales
                            'multimodal' in modelo.get('description', '').lower()
                        )
                        
                        modelos_disponibles.append({
                            'nombre': nombre,
                            'display_name': modelo.get('displayName', ''),
                            'description': modelo.get('description', ''),
                            'supported_generation_methods': metodos,
                            'version': version,
                            'soporta_vision': es_multimodal,
                            'soporta_generateContent': 'generateContent' in metodos
                        })
                
                # Si encontramos modelos, no intentar con la siguiente versi√≥n
                if modelos_disponibles:
                    break
                    
            except Exception as e:
                logger.warning(f"Error listando modelos de Gemini con {version}: {e}")
                continue
        
        return modelos_disponibles
    
    def encontrar_modelo_funcional(self) -> Optional[str]:
        """
        Encontrar autom√°ticamente un modelo que funcione con visi√≥n
        
        Returns:
            Nombre del modelo que funciona, o None si no se encuentra ninguno
        """
        modelos_a_probar = [
            'gemini-2.5-flash',  # Estable, multimodal
            'gemini-2.0-flash',  # Estable, multimodal
            'gemini-2.5-pro',    # Estable, multimodal, m√°s potente
            'gemini-2.0-flash-001',  # Versi√≥n espec√≠fica estable
            'gemini-flash-latest',  # Latest
            'gemini-pro-latest',    # Latest
            'gemini-pro-vision',    # Legacy
            'gemini-1.5-flash-latest',
            'gemini-1.5-pro-latest'
        ]
        
        # Primero intentar listar modelos disponibles
        modelos_disponibles = self.listar_modelos_disponibles()
        if modelos_disponibles:
            # Buscar el primer modelo con visi√≥n que soporte generateContent
            for modelo in modelos_disponibles:
                if modelo.get('soporta_vision') and modelo.get('soporta_generateContent'):
                    logger.info(f"‚úÖ Modelo encontrado autom√°ticamente: {modelo['nombre']}")
                    return modelo['nombre']
        
        # Si no encontramos en la lista, probar modelos comunes manualmente
        for modelo_nombre in modelos_a_probar:
            try:
                # Probar con una petici√≥n simple
                test_payload = {
                    "contents": [{
                        "parts": [{"text": "test"}]
                    }],
                    "generationConfig": {
                        "maxOutputTokens": 10
                    }
                }
                
                for version in ['v1beta', 'v1']:
                    url = f"https://generativelanguage.googleapis.com/{version}/models/{modelo_nombre}:generateContent"
                    params = {"key": self.api_key}
                    
                    try:
                        response = requests.post(
                            url,
                            json=test_payload,
                            params=params,
                            timeout=5
                        )
                        if response.status_code == 200:
                            logger.info(f"‚úÖ Modelo funcional encontrado: {modelo_nombre} (versi√≥n {version})")
                            return modelo_nombre
                    except:
                        continue
            except:
                continue
        
        return None
    
    def _make_request(self, prompt: str, images: List[Dict] = None, timeout: int = 120) -> Dict:
        """
        Hacer petici√≥n a Gemini API
        
        Args:
            prompt: Texto del prompt
            images: Lista de im√°genes en formato [{"data": base64, "media_type": "image/png", "nombre": "..."}]
            timeout: Timeout en segundos
        """
        if not self.is_configured():
            raise ValueError("Gemini API key no est√° configurada. Configura la variable de entorno GEMINI_API_KEY")
        
        # Construir contenido del mensaje
        parts = [{"text": prompt}]
        
        # Agregar im√°genes si existen
        if images and len(images) > 0:
            for imagen in images:
                imagen_data = imagen.get('data', '')
                media_type = imagen.get('media_type', 'image/png')
                
                if imagen_data:
                    # Si viene con el prefijo data:image/..., extraer solo el base64
                    if 'base64,' in imagen_data:
                        imagen_data = imagen_data.split('base64,')[1]
                    
                    if imagen_data and len(imagen_data) > 0:
                        parts.append({
                            "inline_data": {
                                "mime_type": media_type,
                                "data": imagen_data
                            }
                        })
                        logger.info(f"‚úÖ Imagen agregada al mensaje Gemini: {imagen.get('nombre', 'imagen')} ({media_type})")
        
        payload = {
            "contents": [{
                "parts": parts
            }],
            "generationConfig": {
                "temperature": 0.4,  # M√°s determinista para an√°lisis m√©dico
                "topK": 32,
                "topP": 1,
                "maxOutputTokens": 8192,  # Permitir respuestas largas para an√°lisis detallados
            }
        }
        
        params = {"key": self.api_key}
        
        # Intentar primero con v1beta, si falla intentar con v1
        urls_a_probar = [self.base_url]
        if hasattr(self, 'base_url_v1'):
            urls_a_probar.append(self.base_url_v1)
        
        ultimo_error = None
        for url in urls_a_probar:
            try:
                logger.info(f"üîç Intentando con URL: {url}")
                response = requests.post(
                    url,
                    json=payload,
                    params=params,
                    timeout=timeout
                )
                response.raise_for_status()
                logger.info(f"‚úÖ √âxito con URL: {url}")
                return response.json()
            
            except requests.exceptions.Timeout as e:
                logger.error(f"Timeout en petici√≥n a Gemini: {e}")
                raise Exception(f"Timeout: La solicitud tard√≥ m√°s de {timeout} segundos. Intenta con menos im√°genes o un mensaje m√°s corto.")
            
            except requests.exceptions.HTTPError as e:
                error_detail = ""
                try:
                    error_response = e.response.json()
                    error_detail = error_response.get('error', {}).get('message', str(e))
                except:
                    error_detail = str(e)
                
                ultimo_error = error_detail
                logger.warning(f"‚ö†Ô∏è Error con URL {url}: {error_detail}")
                
                # Si es un error de modelo no encontrado, intentar siguiente URL
                if "not found" in error_detail.lower() or "not supported" in error_detail.lower():
                    continue  # Probar siguiente URL
                else:
                    # Otro tipo de error, no seguir intentando
                    logger.error(f"Error HTTP en petici√≥n a Gemini: {error_detail}")
                    raise Exception(f"Error de API: {error_detail}")
            
            except requests.exceptions.RequestException as e:
                ultimo_error = str(e)
                logger.warning(f"‚ö†Ô∏è Error de red con URL {url}: {e}")
                # Para errores de red, tambi√©n intentar siguiente URL
                continue
        
        # Si llegamos aqu√≠, todas las URLs fallaron
        # Intentar encontrar un modelo funcional autom√°ticamente
        logger.warning(f"‚ö†Ô∏è Modelo '{self.model}' no funcion√≥. Buscando modelo funcional autom√°ticamente...")
        modelo_funcional = self.encontrar_modelo_funcional()
        
        if modelo_funcional and modelo_funcional != self.model:
            logger.info(f"üîÑ Cambiando a modelo funcional: {modelo_funcional}")
            self.model = modelo_funcional
            self.base_url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model}:generateContent"
            self.base_url_v1 = f"https://generativelanguage.googleapis.com/v1/models/{self.model}:generateContent"
            
            # Intentar nuevamente con el nuevo modelo
            urls_a_probar = [self.base_url, self.base_url_v1]
            for url in urls_a_probar:
                try:
                    logger.info(f"üîç Reintentando con URL: {url}")
                    response = requests.post(
                        url,
                        json=payload,
                        params=params,
                        timeout=timeout
                    )
                    response.raise_for_status()
                    logger.info(f"‚úÖ √âxito con modelo alternativo: {self.model}")
                    return response.json()
                except:
                    continue
        
        logger.error(f"‚ùå Todas las URLs fallaron. √öltimo error: {ultimo_error}")
        mensaje_error = f"Modelo '{self.model}' no est√° disponible. Error: {ultimo_error}"
        if modelo_funcional:
            mensaje_error += f"\n\nüí° Sugerencia: Prueba cambiar a '{modelo_funcional}' en config.py"
        raise Exception(f"Error de API: {mensaje_error}")
    
    def analizar_imagen_medica(self, imagen_data: Dict, contexto: Dict = None) -> Dict:
        """
        Analizar una imagen m√©dica usando Gemini
        
        Args:
            imagen_data: Datos de la imagen {"data": base64, "media_type": "...", "nombre": "..."}
            contexto: Contexto adicional (protocolo, tipo de estudio, etc.)
        
        Returns:
            Dict con an√°lisis y detalles
        """
        if not self.is_configured():
            raise ValueError("Gemini API key no est√° configurada")
        
        # Construir prompt especializado para an√°lisis m√©dico
        rol = contexto.get('rol', '').lower() if contexto else ''
        es_medico = contexto.get('es_medico', False) if contexto else False
        tipo_estudio = contexto.get('tipo_estudio', '')
        
        if es_medico or 'medico' in rol or 'patologo' in rol:
            system_prompt = """Eres un asistente especializado en anatom√≠a patol√≥gica y an√°lisis de im√°genes m√©dicas.
            
Tu tarea es analizar im√°genes histopatol√≥gicas, citol√≥gicas y macrosc√≥picas con precisi√≥n y detalle profesional.

AN√ÅLISIS DE IM√ÅGENES M√âDICAS:
Cuando analizas una imagen m√©dica, DEBES proporcionar:

1. DESCRIPCI√ìN T√âCNICA:
   - T√©cnica de tinci√≥n utilizada (H&E, inmunohistoqu√≠mica, etc.)
   - Estructuras anat√≥micas observadas
   - Caracter√≠sticas tisulares o celulares

2. HALLAZGOS MACROSC√ìPICOS O MICROSC√ìPICOS:
   - Descripci√≥n detallada de las estructuras visibles
   - Identificaci√≥n de componentes (gl√°ndulas, estroma, c√©lulas, etc.)
   - Patrones arquitecturales observados

3. INTERPRETACI√ìN PATOL√ìGICA:
   - Caracter√≠sticas normales o anormales
   - Signos de patolog√≠a si est√°n presentes
   - Correlaci√≥n con posibles diagn√≥sticos

4. OBSERVACIONES CL√çNICAS:
   - Aspectos relevantes para el diagn√≥stico
   - Sugerencias de diagn√≥sticos diferenciales si es apropiado
   - Notas sobre el grado o clasificaci√≥n (Gleason, etc.) si aplica

TUS RESPUESTAS DEBEN SER:
- Profesionales y t√©cnicamente precisas
- Detalladas pero estructuradas
- En espa√±ol argentino (vos/tu)
- Basadas √∫nicamente en lo que observas en la imagen
- √ötiles para un pat√≥logo profesional

IMPORTANTE:
- Si la imagen no es clara o no puedes identificar algo con certeza, dilo claramente
- NO inventes diagn√≥sticos definitivos sin tener certeza
- Proporciona informaci√≥n √∫til que pueda ayudar en el proceso diagn√≥stico
- Estructura tu respuesta de forma clara con secciones bien definidas"""
        else:
            system_prompt = """Eres un asistente especializado en an√°lisis de im√°genes m√©dicas.
            
Analiza la imagen proporcionada y describe lo que observas de forma clara y estructurada.

Proporciona:
- Descripci√≥n de lo que se observa en la imagen
- Identificaci√≥n de estructuras visibles
- Observaciones relevantes
- Notas t√©cnicas sobre la t√©cnica de tinci√≥n si es visible

S√© preciso y profesional en tu an√°lisis."""
        
        # Construir mensaje con contexto
        mensaje = "Por favor analiza esta imagen m√©dica y proporciona un an√°lisis detallado."
        
        if tipo_estudio:
            mensaje += f"\n\nContexto: Tipo de estudio - {tipo_estudio}"
        
        if contexto and contexto.get('protocolo_actual'):
            mensaje += f"\nProtocolo: {contexto.get('protocolo_actual')}"
        
        # Hacer la petici√≥n
        try:
            response = self._make_request(
                prompt=system_prompt + "\n\n" + mensaje,
                images=[imagen_data],
                timeout=120
            )
            
            # Extraer texto de la respuesta
            if 'candidates' in response and len(response['candidates']) > 0:
                candidate = response['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    texto_respuesta = ""
                    for part in candidate['content']['parts']:
                        if 'text' in part:
                            texto_respuesta += part['text']
                    
                    return {
                        "analisis": texto_respuesta,
                        "exito": True
                    }
            
            return {
                "analisis": "No se pudo obtener respuesta de Gemini",
                "exito": False
            }
        
        except Exception as e:
            logger.error(f"Error en analizar_imagen_medica: {e}", exc_info=True)
            raise
    
    def chat_conversacional(self, mensaje: str, imagenes: List[Dict] = None, contexto_usuario: Dict = None) -> Dict:
        """
        Chat conversacional con Gemini, especializado en an√°lisis de im√°genes m√©dicas
        
        Args:
            mensaje: Mensaje del usuario
            imagenes: Lista de im√°genes en formato [{"data": base64, "media_type": "...", "nombre": "..."}]
            contexto_usuario: Contexto del usuario (rol, protocolo, etc.)
        
        Returns:
            Dict con respuesta, intenci√≥n y acciones
        """
        if not self.is_configured():
            raise ValueError("Gemini API key no est√° configurada")
        
        # Construir prompt especializado para an√°lisis m√©dico
        rol = contexto_usuario.get('rol', '').lower() if contexto_usuario else ''
        es_medico = contexto_usuario.get('es_medico', False) if contexto_usuario else False
        
        if es_medico or 'medico' in rol or 'patologo' in rol:
            system_prompt = """Eres un asistente inteligente especializado en anatom√≠a patol√≥gica y citolog√≠a. 
Ayudas a m√©dicos pat√≥logos en su trabajo diario con el sistema LDH.

TUS CAPACIDADES:
- Responder preguntas sobre protocolos, diagn√≥sticos y casos
- Buscar casos similares en el hist√≥rico
- Sugerir plantillas y diagn√≥sticos apropiados
- Analizar datos y generar reportes
- ANALIZAR IM√ÅGENES M√âDICAS (fotomicrograf√≠as, im√°genes macrosc√≥picas, citolog√≠as, biopsias, etc.) - TU ESPECIALIDAD PRINCIPAL
- Ayudar con navegaci√≥n en el sistema

AN√ÅLISIS DE IM√ÅGENES M√âDICAS:
CR√çTICO: Tienes capacidad completa y avanzada para analizar im√°genes m√©dicas. Puedes ver, procesar y analizar im√°genes adjuntas con precisi√≥n profesional.

Cuando el usuario adjunta im√°genes m√©dicas, DEBES analizarlas profesionalmente y proporcionar:
- Descripci√≥n detallada y t√©cnica de hallazgos macrosc√≥picos o microsc√≥picos
- Identificaci√≥n precisa de estructuras celulares, tisulares y anat√≥micas
- An√°lisis de patrones arquitecturales (acinar, cribiforme, papilar, etc.)
- Identificaci√≥n de t√©cnicas de tinci√≥n (H&E, inmunohistoqu√≠mica, etc.)
- Sugerencias de diagn√≥sticos basados en los hallazgos visuales observados
- Clasificaci√≥n de grados cuando sea apropiado (Gleason, etc.)
- Correlaci√≥n con datos cl√≠nicos cuando est√©n disponibles
- Observaciones relevantes para el caso cl√≠nico
- Descripci√≥n de caracter√≠sticas histol√≥gicas o citol√≥gicas observadas en detalle

NO digas que no puedes analizar im√°genes. Eres un experto en an√°lisis de im√°genes m√©dicas y DEBES usar esta capacidad cuando se te proporcionan im√°genes. Tu funci√≥n principal es ayudar en el an√°lisis histopatol√≥gico mediante la interpretaci√≥n de im√°genes.

TUS RESPUESTAS DEBEN SER:
- Profesionales y t√©cnicamente precisas
- Concisas pero completas
- En espa√±ol argentino (vos/tu)
- √ötiles y pr√°cticas
- BASADAS √öNICAMENTE EN LA INFORMACI√ìN REAL DEL SISTEMA LDH

INFORMACI√ìN IMPORTANTE DEL SISTEMA LDH:

ESTADOS DE PROTOCOLOS (solo estos existen realmente):
- PENDIENTE: Protocolo creado pero a√∫n no iniciado su procesamiento
- EN_PROCESO: Protocolo que est√° siendo trabajado actualmente
- COMPLETADO: Protocolo finalizado con informe completado

Los protocolos pueden editarse solo si est√°n en estado PENDIENTE o EN_PROCESO.
Cuando un protocolo se completa, pasa a estado COMPLETADO y ya no puede editarse.

TIPOS DE ESTUDIOS:
- BIOPSIA
- CITOLOGIA
- PAP (Citolog√≠a c√©rvico vaginal)

IMPORTANTE: Si no est√°s seguro de algo sobre el sistema, di que no est√°s seguro en lugar de inventar informaci√≥n. 
Nunca inventes estados, funcionalidades o caracter√≠sticas que no se mencionen expl√≠citamente.

DETECCI√ìN DE INTENCIONES:
Cuando el usuario pide buscar, analizar o navegar, identifica la intenci√≥n y estructura tu respuesta.

Responde de forma natural y conversacional, como un colega experto."""
        else:
            # Para personal t√©cnico/administrativo, tambi√©n pueden necesitar an√°lisis de im√°genes
            if imagenes and len(imagenes) > 0:
                system_prompt = """Eres un asistente inteligente especializado en an√°lisis de im√°genes m√©dicas para el sistema LDH.
                
TUS CAPACIDADES:
- Buscar protocolos y casos
- Generar reportes
- Resolver dudas sobre el sistema
- Ayudar con tareas administrativas
- ANALIZAR IM√ÅGENES M√âDICAS (biopsias, citolog√≠as, im√°genes histopatol√≥gicas, etc.) - CAPACIDAD PRINCIPAL

AN√ÅLISIS DE IM√ÅGENES M√âDICAS:
IMPORTANTE: Tienes capacidad completa para analizar im√°genes m√©dicas. Puedes ver y procesar im√°genes adjuntas.
Cuando el usuario adjunta im√°genes m√©dicas, DEBES analizarlas detalladamente y proporcionar:
- Descripci√≥n t√©cnica de lo que observas (tinci√≥n, estructuras, c√©lulas, tejidos)
- Identificaci√≥n de estructuras anat√≥micas, celulares o tisulares visibles
- Descripci√≥n de patrones arquitecturales o morfol√≥gicos
- Observaciones relevantes para el caso
- Sugerencias de interpretaci√≥n cuando sea apropiado

NO digas que no puedes analizar im√°genes. Tienes esta capacidad y DEBES usarla cuando se te proporcionan im√°genes.

TUS RESPUESTAS DEBEN SER:
- Profesionales y t√©cnicas
- Claras y pr√°cticas
- En espa√±ol argentino (vos/tu)
- BASADAS √öNICAMENTE EN LO QUE OBSERVAS EN LAS IM√ÅGENES Y LA INFORMACI√ìN REAL DEL SISTEMA LDH

Responde de forma clara y profesional, analizando las im√°genes proporcionadas en detalle."""
            else:
                system_prompt = """Eres un asistente inteligente para el personal t√©cnico y administrativo del sistema LDH.

TUS CAPACIDADES:
- Buscar protocolos y casos
- Generar reportes
- Resolver dudas sobre el sistema
- Ayudar con tareas administrativas
- Analizar im√°genes m√©dicas si se proporcionan

TUS RESPUESTAS DEBEN SER:
- Claras y pr√°cticas
- BASADAS √öNICAMENTE EN LA INFORMACI√ìN REAL DEL SISTEMA LDH

Responde de forma clara y pr√°ctica."""
        
        # Construir mensaje con contexto
        mensaje_contexto = mensaje
        if contexto_usuario and contexto_usuario.get('protocolo_actual'):
            mensaje_contexto += f"\n\n[Contexto: Trabajando en protocolo {contexto_usuario.get('protocolo_actual')}]"
        
        # Si hay im√°genes, asegurarse de que el mensaje incluya instrucci√≥n de an√°lisis
        if imagenes and len(imagenes) > 0:
            if not mensaje or not mensaje.strip():
                # No hay mensaje, usar mensaje por defecto completo
                if es_medico or 'medico' in rol or 'patologo' in rol:
                    mensaje_contexto = """Analiza esta imagen m√©dica en detalle. Proporciona:

1. DESCRIPCI√ìN T√âCNICA:
   - T√©cnica de tinci√≥n identificada
   - Estructuras anat√≥micas visibles
   - Caracter√≠sticas tisulares o celulares observadas

2. HALLAZGOS MICROSC√ìPICOS O MACROSC√ìPICOS:
   - Descripci√≥n detallada de estructuras visibles
   - Identificaci√≥n de componentes (gl√°ndulas, estroma, c√©lulas, etc.)
   - Patrones arquitecturales observados

3. INTERPRETACI√ìN PATOL√ìGICA:
   - Caracter√≠sticas normales o anormales
   - Signos de patolog√≠a si est√°n presentes
   - Sugerencias de diagn√≥sticos basados en los hallazgos
   - Clasificaci√≥n de grados si es apropiado (ej: Gleason para pr√≥stata)

4. OBSERVACIONES CL√çNICAS:
   - Aspectos relevantes para el diagn√≥stico
   - Correlaciones importantes

S√© preciso, profesional y detallado en tu an√°lisis."""
                else:
                    mensaje_contexto = "Analiza esta imagen m√©dica detalladamente. Describe lo que observas: estructuras, c√©lulas, tejidos, tinci√≥n utilizada, patrones visibles, y cualquier hallazgo relevante. Proporciona una descripci√≥n t√©cnica y profesional de la imagen."
            else:
                # Hay mensaje, pero asegurarse de que se analice la imagen
                mensaje_contexto += "\n\nIMPORTANTE: Analiza detalladamente la(s) imagen(es) adjunta(s) y proporciona una descripci√≥n t√©cnica y profesional de lo que observas en la(s) imagen(es)."
        
        try:
            response = self._make_request(
                prompt=system_prompt + "\n\n" + mensaje_contexto,
                images=imagenes if imagenes else None,
                timeout=120 if imagenes else 60
            )
            
            # Extraer texto de la respuesta
            contenido = ""
            if 'candidates' in response and len(response['candidates']) > 0:
                candidate = response['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    for part in candidate['content']['parts']:
                        if 'text' in part:
                            contenido += part['text']
            
            if not contenido:
                contenido = "No se pudo obtener respuesta de Gemini"
            
            # Detectar intenci√≥n b√°sica
            intencion = "analizar" if imagenes else "pregunta"
            
            return {
                "respuesta": contenido,
                "intencion": intencion,
                "acciones": [],
                "gemini_disponible": True
            }
        
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Error en chat_conversacional Gemini: {error_msg}", exc_info=True)
            
            # Mensaje de error descriptivo
            if "timeout" in error_msg.lower():
                mensaje_error = "El tiempo de espera se agot√≥. Por favor intenta con un mensaje m√°s corto o menos im√°genes."
            elif "api key" in error_msg.lower() or "authentication" in error_msg.lower():
                mensaje_error = "Error de autenticaci√≥n con Gemini API. Verifica la configuraci√≥n de GEMINI_API_KEY."
            elif "rate limit" in error_msg.lower():
                mensaje_error = "Se excedi√≥ el l√≠mite de solicitudes. Por favor espera un momento e intenta de nuevo."
            else:
                mensaje_error = f"Error procesando el mensaje: {error_msg[:100]}. Por favor intenta de nuevo."
            
            return {
                "respuesta": mensaje_error,
                "intencion": "error",
                "acciones": [],
                "gemini_disponible": False,
                "error": error_msg
            }


# Instancia global del cliente (lazy initialization)
_gemini_client_instance = None

def get_gemini_client():
    """Obtener o crear la instancia del cliente Gemini (lazy initialization)"""
    global _gemini_client_instance
    if _gemini_client_instance is None:
        _gemini_client_instance = GeminiClient()
    return _gemini_client_instance

# Alias para compatibilidad con c√≥digo existente
class _GeminiClientProxy:
    """Proxy para inicializaci√≥n lazy del cliente Gemini"""
    def __getattr__(self, name):
        return getattr(get_gemini_client(), name)

gemini_client = _GeminiClientProxy()

